@article{xiao2023efficient,
  title={Efficient Streaming Language Models with Attention Sinks},
  author={Xiao, Guangxuan and Tian, Yuandong and Chen, Beidi and Han, Song and Lewis, Mike},
  journal={arXiv preprint, arXiv:2309.17453},
  year={2023},
  url={https://arxiv.org/pdf/2309.17453.pdf}
}

@article{barbero2025why,
  title={Why do LLMs attend to the first token?},
  author={Barbero, Federico and Arroyo, Álvaro and Gu, Xiangming and Perivolaropoulos, Christos and Bronstein, Michael and Veličković, Petar and Pascanu, Razvan},
  journal={arXiv preprint, arXiv:2504.02732},
  year={2025},
  url={https://arxiv.org/pdf/2504.02732.pdf}
}

@article{zuhri2025softpick,
  title={Softpick: No Attention Sink, No Massive Activations with Rectified Softmax},
  author={Zuhri, Zayd M. K. and Fuadi, Erland Hilman and Aji, Alham Fikri},
  journal={arXiv preprint, arXiv:2504.20966},
  year={2025},
  url={https://arxiv.org/pdf/2504.20966.pdf}
}

@article{jiang2024minference,
  title={MInference 1.0: Accelerating Pre-filling for Long-Context LLMs via Dynamic Sparse Attention},
  author={Jiang, Huiqiang and Li, Yucheng and Zhang, Chengruidong and Wu, Qianhui and Luo, Xufang and Ahn, Surin and Han, Zhenhua and Abdi, Amir H. and Li, Dongsheng and Lin, Chin-Yew and Yang, Yuqing and Qiu, Lili},
  journal={arXiv preprint, arXiv:2407.02490},
  year={2024},
  url={https://arxiv.org/pdf/2407.02490.pdf}
}

@article{liu2024retrievalattention,
  title={RetrievalAttention: Accelerating Long-Context LLM Inference via Vector Retrieval},
  author={Liu, Di and Chen, Meng and Lu, Baotong and Jiang, Huiqiang and Han, Zhenhua and Zhang, Qianxi and Chen, Qi and Zhang, Chengruidong and Ding, Bailu and Zhang, Kai and Chen, Chen and Yang, Fan and Yang, Yuqing and Qiu, Lili},
  journal={arXiv preprint, arXiv:2409.10516},
  year={2024},
  url={https://arxiv.org/pdf/2409.10516.pdf}
}

@article{xu2025xattention,
  title={XAttention: Block Sparse Attention with Antidiagonal Scoring},
  author={Xu, Ruyi and Xiao, Guangxuan and Huang, Haofeng and Guo, Junxian and Han, Song},
  journal={arXiv preprint, arXiv:2503.16428},
  year={2025},
  url={https://arxiv.org/pdf/2503.16428.pdf}
}

@article{yang2024tidaldecode,
  title={TidalDecode: Fast and Accurate LLM Decoding with Position Persistent Sparse Attention},
  author={Yang, Lijie and Zhang, Zhihao and Chen, Zhuofu and Li, Zikun and Jia, Zhihao},
  journal={arXiv preprint, arXiv:2410.05076},
  year={2024},
  url={https://arxiv.org/pdf/2410.05076.pdf}
}

@article{yuan2025native,
  title={Native Sparse Attention: Hardware-Aligned and Natively Trainable Sparse Attention},
  author={Yuan, Jingyang and Gao, Huazuo and Dai, Damai and Luo, Junyu and Zhao, Liang and Zhang, Zhengyan and Xie, Zhenda and Wei, Y. X. and Wang, Lean and Xiao, Zhiping and Wang, Yuqing and Ruan, Chong and Zhang, Ming and Liang, Wenfeng and Zeng, Wangding},
  journal={arXiv preprint, arXiv:2502.11089},
  year={2025},
  url={https://arxiv.org/pdf/2502.11089.pdf}
}

@article{gao2024seerattention,
  title={SeerAttention: Learning Intrinsic Sparse Attention in Your LLMs},
  author={Gao, Yizhao and Zeng, Zhichen and Du, Dayou and Cao, Shijie and Zhou, Peiyuan and Qi, Jiaxing and Lai, Junjie and So, Hayden Kwok-Hay and Cao, Ting and Yang, Fan and Yang, Mao},
  journal={arXiv preprint, arXiv:2410.13276},
  year={2024},
  url={https://arxiv.org/pdf/2410.13276.pdf}
}

@article{hooper2024squeezed,
  title={Squeezed Attention: Accelerating Long Context Length LLM Inference},
  author={Hooper, Coleman and Kim, Sehoon and Mohammadzadeh, Hiva and Maheswaran, Monishwaran and Zhao, Sebastian and Paik, June and Mahoney, Michael W. and Keutzer, Kurt and Gholami, Amir},
  journal={arXiv preprint, arXiv:2411.09688},
  year={2024},
  url={https://arxiv.org/pdf/2411.09688.pdf}
}

@article{desai2024hashattention,
  title={HashAttention: Semantic Sparsity for Faster Inference},
  author={Desai, Aditya and Yang, Shuo and Cuadron, Alejandro and Zaharia, Matei and Gonzalez, Joseph E. and Stoica, Ion},
  journal={arXiv preprint, arXiv:2412.14468},
  year={2024},
  url={https://arxiv.org/pdf/2412.14468.pdf}
}

@article{kim2025epicache,
  title={EpiCache: Episodic KV Cache Management for Long Conversational Question Answering},
  author={Kim, Minsoo and Kundu, Arnav and Kim, Han-Byul and Dixit, Richa and Cho, Minsik},
  journal={arXiv preprint, arXiv:2509.17396},
  year={2025},
  url={https://arxiv.org/pdf/2509.17396.pdf}
}

@article{zhang2023h2o,
  title={H$_2$O: Heavy-Hitter Oracle for Efficient Generative Inference of Large Language Models},
  author={Zhang, Zhenyu and Sheng, Ying and Zhou, Tianyi and Chen, Tianlong and Zheng, Lianmin and Cai, Ruisi and Song, Zhao and Tian, Yuandong and R{\'e}, Christopher and Barrett, Clark and Wang, Zhangyang and Chen, Beidi},
  journal={arXiv preprint, arXiv:2306.14048},
  year={2023},
  url={https://arxiv.org/pdf/2306.14048.pdf}
}