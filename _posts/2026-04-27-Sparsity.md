---
layout: distill
title: Sparsity
description:
  Large Language Models (LLMs) have brought about a major change in artificial intelligence. They have transitioned in scope from being specialized research tools to common resources that drive the next generation of software. By increasing model parameters and training data, LLMs show new abilities in reasoning, code generation, and solving complex problems that were once considered unattainable. Scaling transformers effectively for long context applications uniquely poses a challenge. This is primarily because the self-attention mechanism has space and time complexity O(N^2). This quadratic bottleneck inhibits applications for long documents, high-resolution images, large codebases, etc. 
  Sparsity emerges as an effective solution to this problem. Rather than relying on the N x N attention matrix, it utilizes an approximate or “sparse” version. The backbone of this approach is the idea that tokens do not require the entire context; they only need local context, and thus, most of the computation being carried out is wasteful.
date: 2026-04-27
future: true
htmlwidgets: true
hidden: true

# Mermaid diagrams
mermaid:
  enabled: true
  zoomable: true

# Anonymize when submitting
# authors:
#   - name: Anonymous

authors:
  - name: Aryan Sood
    url: 
    affiliations:
      name: IIT Roorkee
  - name: Tanvi Sharma
    url: 
    affiliations:
      name: IIT Roorkee
  - name: Vansh Agrawal
    url: 
    affiliations:
      name: IIT Roorkee

# must be the exact same name as your blogpost
bibliography: 2026-04-27-Sparsity.bib

# Add a table of contents to your post.
#   - make sure that TOC names match the actual section names
#     for hyperlinks within the post to work correctly.
#   - please use this format rather than manually creating a markdown table of contents.
toc:
  - name: Background
  - name: Preliminaries
  - name: Introduction
  - name: Attention Sinks
    subsections:
      - name: The Problem
      - name: How it works
      - name: Implications on Sparse Attention
  - name: Beyond Softmax
    subsections:
      - name: Softpick
  - name: Attention Patterns
    subsections:
      - name: Block Based Computation
      - name: Dynamic Sparsity
      - name: Hardware Aware Methods
  - name: Dynamic KV Cache Management
    subsections:
      - name: Squeezed Attention
      - name: Epicache
        subsections:
          - name: Methodology
            subsections:
              - name: Stage 1
              - name: Stage 2
              - name: Stage 3
      - name: Dynamic Decoding
        subsections:
        - name: Heavy Hitters
        - name: Eviction
        - name: Theoretical formulation 
  - name: Conclusion

# Below is an example of injecting additional post-specific styles.
# This is used in the 'Layouts' section of this post.
# If you use this post as a template, delete this _styles block.
_styles: >
  .fake-img {
    background: #bbb;
    border: 1px solid rgba(0, 0, 0, 0.1);
    box-shadow: 0 0px 4px rgba(0, 0, 0, 0.1);
    margin-bottom: 12px;
  }
  .fake-img p {
    font-family: monospace;
    color: white;
    text-align: left;
    margin: 12px 0;
    text-align: center;
    font-size: 16px;
  }
---

Note: please use the table of contents as defined in the front matter rather than the traditional markdown styling.

## Background

The foundation of modern Large Language Models is the Self-Attention mechanism, first introduced in *Attention Is All You Need* (Vaswani et al., 2017). This process allows the model to weigh the importance of different tokens relative to one another. It achieves this by mapping inputs to Query ($Q$), Key ($K$), and Value ($V$) matrices to compute a weighted sum.


In normal autoregressive generation (predicting one token at a time), re-computing the Key and Value matrices for the entire history at every step is inefficient. To solve this, we utilize KV Caching. By storing the $K$ and $V$ states of previous tokens in GPU memory, the model only needs to compute attention for the *current* token thereby speeding up inference.

While KV caching avoids re-computation, the memory and operations required for the attention mechanism still scale poorly. Because every token must attend to every other token, the complexity grows quadratically with the sequence length. This $O(N^2)$ cost creates a bottleneck, causing memory usage to explode as the context window increases.

## Preliminaries

### Simple Attention

Mechanism
Simple attention processes the entire sequence simultaneously, such that every element attends to every other element. For a sequence of length $L$ and embedding dimension $d$, the input is represented as a matrix $X \in \mathbb{R}^{L \times d}$. The mechanism uses learnable weight matrices $W_Q, W_K, W_V \in \mathbb{R}^{d \times d_{model}}$ to project the input.

#### Formulation
The attention matrix is computed globally for the full sequence:$$\text{Attention}(Q, K) = \text{softmax}\left( \frac{$$$$Z = \text{Attention$$For individual tokens, the attention score $a_{m,n}$ represents the relevance of token $n$ (key) to token $m$ (query):$$a$$This mechanism has a computational complexity of $O(L^2)$, as it computes interactions between all $L \times L$ pairs.

---

### Layerwise Attention

#### Mechanism
Unlike simple attention which operates within a single layer, Layerwise Attention (often referred to as Vertical Attention) enables the model to capture dependencies across different layers of the network. Instead of attending only to the current layer's input, the queries at layer $l$ can attend to keys and values aggregated from previous layers or a set of memory states from the entire depth of the network.

#### Formulation
Let $H^{(l)} \in \mathbb{R}^{L \times d}$ be the representation at layer $l$. The queries are derived from the current layer $H^{(l)}$, while keys and values may be derived from a set of previous layers $\{H^{(0)}, \dots, H^{(l)}\}$:
$$
Q^{(l)} = H^{(l)} W_Q^{(l)}
$$
$$
K$$
$$
V_{all}^{(l)} = \text{Concat}(H^{(0
$$
The attention scores are computed over the expanded set of keys:
$$
\text{Attention}(Q^{(l)}, K_{all}^{(l)}) =
$$
This allows the context vector $x_i$ to directly incorporate information from lower-level features or original embeddings, improving gradient flow and feature propagation.

### Blockwise Attention

#### Mechanism
Blockwise Attention addresses the quadratic complexity of simple attention by partitioning the sequence into smaller, non-overlapping blocks (or chunks) of length $B$. The attention mechanism is restricted to interactions within these blocks or between specific blocks (e.g., local windows), making the computation more efficient for long sequences.

#### Formulation
The input matrices $Q, K, V$ are partitioned into $N_B = L/B$ blocks: $Q = [Q_1, Q_2, \dots, Q_{N_B}]$, and similarly for $K$ and $V$, where $Q_i \in \mathbb{R}^{B \times d_{model}}$.Attention is computed locally for each block $i$:
$$
\text{BlockAttention}(Q_i, K_i) =
$$
$$
Z_i = \text{BlockAttention}(Q_i, K
$$
The final output $Z$ is the concatenation of block outputs:
$$
Z = \text{Concat}(Z_1, \dots, Z_{N_B})
$$
For a token $m$ in block $b$, the attention score $a_{m,n}$ is non-zero only if token $n$ also falls within the defined block or neighborhood (e.g., $n \in \text{Block}_b$), reducing complexity from $O(L^2)$ to $O(L \cdot B)$.

## Introduction

Large Language Models (LLMs) have brought about a major change in artificial intelligence. They have transitioned in scope from being specialized research tools to common resources that drive the next generation of software. By increasing model parameters and training data, LLMs show new abilities in reasoning, code generation, and solving complex problems that were once considered unattainable. Scaling transformers effectively for long context applications uniquely poses a challenge. This is primarily because the self-attention mechanism has space and time complexity $\mathcal{O}\left(n^2\right)$. This quadratic bottleneck inhibits applications for long documents, high-resolution images, large codebases, etc. 

Sparsity emerges as an effective solution to this problem. Rather than relying on the N x N attention matrix, it utilizes an approximate or “sparse” version. The backbone of this approach is the idea that tokens do not require the entire context; they only need local context, and thus, most of the computation being carried out is wasteful.

{% include figure.liquid path="assets/img/2026-04-27-Sparsity/1.png" class="img-fluid" caption="Figure 1: Attention heatmaps showing naturally sparse patterns in dense models. Even with full attention budget, models learn to concentrate attention on specific tokens rather than distributing it uniformly." %}

We can verify this hypothesis by examining the token heatmaps. When we visualize the attention layers of standard, dense models, they often reveal a fascinating insight: even when given the budget to look everywhere, these models learn naturally sparsSe patterns. Attention is not uniformly distributed, but rather concentrated on a few specific tokens. This finding serves as the foundation for the efficacy of sparse attention methods. If regular self attention is inherently sparse in long contexts, then methods that rely on sparse attention serve as a more efficient architecture that relies on established patterns.

---

## Attention Sinks

From the heatmaps displayed above, you would consistently notice the following strange behavior: many of their attention heads focus intensely on the very first token in a sequence. This token is often a special <bos> token that carries little semantic meaning. Even in vast models like LLaMa 3.1 405B, a staggering 80% of the attention heads exhibit this pattern, directing most of their focus to this single starting token.

At first, this seems like an incredible inefficiency, almost as if the model is wasting its computation. However, recent contributions and publications argue that this tendency is not a flaw but rather a way to overcome a fundamental flaw in an LLM’s architecture, and this behaviour is what is termed as attention sinks.


### The Problem

The core issue addressed by attention sinks is "over-mixing". A Transformer operates on the principle of repeatedly mixing information between tokens at every layer. While this is a necessary principle for understanding context, it becomes a problem in very deep models or when processing long sequences or a large context.

After too many layers of mixing, the unique information for each token can become "smoothened out", and all the tokens start to look the same to the model. This exhibits two phenomena:

1. **Rank collapse** - representations of all tokens in a sequence become too similar to each other as they pass through the model's layers.
2. **Over-smoothing** - the same phenomenon as rank collapse, except the term is typically used when discussing Graph Neural Networks (GNNs). 

When this information blur happens, the model can no longer distinguish between tokens effectively, which harms its ability to make accurate predictions.
Attention Sinks provide a simple yet effective solution to this; they act as a “do-nothing” option.

### How it works

1. **The Sink as a neutral target:** The model learns that the first token is a reliable, ever-present, and neutral target.
2. **Low-Information value:** The value vector associated with this token is often learned to have a very small norm, indicating that it contains very little information.
3. **Skipping the update:** When an attention head wants to avoid mixing more information into a token, it simply directs all its attention to the sink. It picks up the low-information value, and when that is added back to the token's representation, it changes it very little (it's like adding zero). The token effectively skips the mixing step in that layer, preserving its distinct information.

This allows the model to dynamically control how much mixing occurs at each layer for each token, preventing the representations from becoming a blurry mess.

### Implications on Sparse Attention

We can conclude that the sink phenomenon is essential for sparse attention methods. An initial and fundamental analysis of this was a standard sliding window approach (i.e, StreamingLLM).

We can describe the method as evicting the earlier KV cache as we move deeper into the context. However, performance is observed to collapse when the model evicts the initial tokens. The StreamingLLM framework modifies the sliding window to always contain the initial tokens. The more stable performance of this hybrid approach confirms the necessity of attention sinks. 

---

## Beyond Softmax

The softmax function is a fundamental part of all transformer architectures. Its usage is the norm for most machine learning algorithms due to how it can normalise a vector of real numbers into a probability distribution. And at the same time, the distribution can be interpreted as the probability of a query matching a key among multiple keys.

Additionally, softmax offers training stability due to its dense gradient and bounded Frobenius Norm, while also helping with non-linearity and regularization. But despite all these advantages, the sum-to-one nature of softmax is also the root cause of “attention sinks”. These sinks are not a problem in downstream tasks. However, as the model scales, we observe that massive hidden state activations appear, which prove to be a problem when trying to quantize the model, due to instability when represented in low precision.

The majority of approaches are still based on standard softmax, but there have been some attempts to reformulate the function itself. Some primitive methods to this end are as follows

1. Using the sigmoid function without normalisation to avoid attention sinks, it was observed that whenever normalisation was reintroduced, sinks reappeared, which proved the direct linkage between the two.
2. Adding an attention bias by adding an extra vector to keys, queries, and values, but this only works if the vectors were trainable, and this was also seen to deteriorate as the model got deeper 
3. Other approaches, which tried to slightly modify the softmax function, needed custom optimisers to avoid outlier activations

### Softpick

Softpick was a mathematical reformulation of the softmax function. It was designed to be a drop-in solution that did not require any extra parameters or custom optimisers. Given the training benefits of softmax, the authors sought to preserve them despite the changes.

This was achieved by maintaining the Jacobian Matrix, thereby keeping the gradient norm bounded. The first innovation was to allow “null attention,” i.e, allowing attention to output 0. This was done by reducing the exponential and rectifying with ReLU.

$$
Softmax(\mathbf{x})_i = \frac{{\color{orange} ReLU}\left(e^{x_i} - 1\right)}{\sum_{j=1}^{N} {\color{orange} ReLU}\left(e^{x_j} - 1\right)}
$$

The problem with this function arose when training was run for longer periods, and the attention heads started “dying”, this was due to the constant outputting of zeros. Also this modification doesn’t get rid of the sinks at all. Additionally, negative inputs did not contribute to the gradient at all.

These issues led to the formulation of the final version:

$$
Softpick(\mathbf{x})_i = \frac{ReLU(e^{x_i} - 1)}{\sum_{j=1}^{N} |e^{x_j} - 1|}
$$

The above formulation allowed the gradient to flow even when the input was negative. This was an issue in the predecessor, and at the same time, this function removes the sum-to-one property of the softmax function, thereby fixing the issue of attention sinks.

Theoretically, softpick would’ve been revolutionary in the field of long context as it made the quantisation process less damaging and helped reduce noise in longer contexts. The main issue is that SoftPick assigns smaller scores the sparser the attention pattern and the longer the context. This is because long contexts often tend to have a massive number of negatively scored tokens, which scales down the attention scores due to the absolute function introduced in the denominator, thereby often being problematic in retrieval. This issue was termed as “underscoring”.

This paper expands the scope of work towards reformulating the softmax function, which was previously regarded as an indispensable component of all transformer architectures. 

---

## Attention Patterns

Although the ideal sparsity pattern of an attention matrix is theoretically unique to every input, empirical analysis reveals that attention often clusters into predictable geometric shapes. Leveraging these observations allows us to reduce computational overhead significantly.

### 1. Observed Static Patterns

Frameworks like MInference utilize specific, recognizable structures in the attention matrix to approximate the full calculation.

{% include figure.liquid path="assets/img/2026-04-27-Sparsity/2.png" class="img-fluid" caption="Figure 2: Common sparsity patterns observed in attention matrices: A-shape (combining local and global attention), Block-sparse (sliding window with global blocks), and Vertical-slash (fixed-stride or dilated attention)." %}

**A-shape**

The "A-shape" pattern is a method for selecting which token interactions are most important. It gets its name from the shape it forms when you visualize the sparse attention matrix. This structure often combines local and global information. For example, tokens may focus completely on their nearby neighbors, creating the wide base of the "A," while also linking to a few carefully chosen "summary" tokens or positions earlier in the context, forming the peak. This “A” structure effectively connects detailed local context with a sparse but important long-range signal, without requiring a full quadratic computation.


**Vertical-slash**

The "vertical-slash" pattern offers another useful method. It gets its name from the way it looks in the attention matrix. This pattern often represents a "fixed-stride" or "dilated" attention mechanism. Instead of focusing on every previous token, which is too costly, or just a local window, which misses the bigger picture, a vertical-slash pattern may have each token attend to every 100th token. This approach lets the model sample the entire context, regardless of its length. It creates a basic structure of long-range dependencies that is very efficient to compute.

**Block-sparse**

Perhaps the most intuitive of these patterns is block-sparse attention. Imagine the full, N×N attention matrix as a massive grid. Instead of calculating the entire grid, block-sparse attention only computes small, specific blocks within it. Most commonly, this includes a sliding window which consists of the blocks along the main diagonal. In this setup, each token only looks at its immediate neighbors. This approach is great for capturing local context. To avoid losing sight of the long-range view, this pattern is often improved by computing a few fixed global blocks. These blocks allow certain groups of tokens to communicate even if they are far apart in the sequence.



### 2. Neighborhood-Based Sparse Attention

While the patterns above are observations, **Neighborhood-Based Sparse Attention** refers to a class of methods that enforce these structured patterns algorithmically to optimize performance.

**Sliding Window Attention**

The most intuitive example of this class is Sliding Window Attention (SWA). Here, the attention matrix is restricted to a band along the diagonal: each query token $q_i$ only attends to a fixed-size window of key tokens $k_j$ where $|i-j| \le w$. This effectively relies on the "locality" heuristic—the assumption that a token's most relevant information is its immediate neighbors.

**The Hardware Advantage**

Methods like SWA, dilated attention, and fixed-block patterns share a key characteristic: **Regularity**. Because the sparsity pattern is known ahead of time (e.g., $i \pm w$), indices can be pre-computed and memory access can be coalesced. This allows for efficient implementation using custom GPU kernels, minimizing the overhead of indirect addressing that plagues unstructured sparsity.

**The Propagation Bottleneck**

However, strict locality introduces a challenge. In SWA, direct interaction between unadjacent tokens (where $|i-j| \gg w$) is prohibited within a single layer. Information must "hop" from block to block across multiple layers to travel the length of the sequence. This multi-hop path causes signal attenuation and hinders the model's ability to capture long-range dependencies effectively.

**Solution: Star Attention**

Star Attention mitigates this propagation issue while maintaining the efficiency of neighborhood-based methods. It adopts a query-centric global topology, assuming that while most information is local, specific "anchor" tokens can act as global summarizers.
A query token computes attention over two sets:
1.  **Local Block:** Its immediate neighbors (Standard SWA).
2.  **Anchor Block:** A shared global summarizer (e.g., the first block) that acts as a hub in a star-graph topology.

This two-phase approach allows any query to access a summarized version of global information immediately, effectively eliminating the need for multi-hop propagation.

### 3. Dynamic Sparsity

The limitation of the methods above is that they are structurally fixed—they apply the same geometric rule regardless of the text. **Dynamic Sparsity** approaches, conversely, generate content-aware sparsity masks.

Here, the attention pattern $S$ is a function of the input itself: $S=f(Q,K,V)$. The model learns which interactions are relevant for the *specific* input and allocates computation accordingly.

**Example: Tidal Attention**

An implemented example of this is Tidal Attention. It works by dynamically adjusting the budget of tokens being attended to. This is implemented in the form of a learned threshold or gating mechanism applied to the attention score matrix $A=QK^T$. After computing the full (or a subset of) scores, the model prunes any $A_{ij}$ score that falls below $\tau$, which is a dynamically computed, sequence-specific threshold. This effectively focuses computation on only the $q_i-k_j$ pairs that have the highest relevance for that specific input.

### 3. Dynamic Sparsity

The limitation of the methods above is that they are structurally fixed—they apply the same geometric rule regardless of the text. **Dynamic Sparsity** approaches, conversely, generate content-aware sparsity masks.

Here, the attention pattern $S$ is a function of the input itself: $S=f(Q,K,V)$. The model learns which interactions are relevant for the *specific* input and allocates computation accordingly.

**Example: Tidal Attention**

An implemented example of this is Tidal Attention. It works by dynamically adjusting the budget of tokens being attended to. This is implemented in the form of a learned threshold or gating mechanism applied to the attention score matrix $A=QK^T$. After computing the full (or a subset of) scores, the model prunes any $A_{ij}$ score that falls below $\tau$, which is a dynamically computed, sequence-specific threshold. This effectively focuses computation on only the $q_i-k_j$ pairs that have the highest relevance for that specific input.

## Dynamic KV Cache Management

In classical Transformers, while computing self-attention scores during the prefilling stage, the norm is to store the KV cache to enable efficient inference, i.e, reducing redundant calculations. However, in cases of long, multiturn conversations, this turns into a significant bottleneck due to the following reasons:

1. The cache grows linearly with context length. In the above case, the cache grows significantly, leading to OOMs.
2. As originally observed, in any context, the attention pattern is sparse, leading us to conclude that for a particular query, parts of the context are entirely irrelevant.

However, in this specific task, we cannot entirely prune the context. This is because the follow-up query of the user may differ from what the original query retrieved, leading to a necessity for dynamic KV cache management, at the same time with the growing context with each query without managing the cache, the model will start hallucinating as the context will simply be too large, and at the same time, the memory footprint stays bounded.

### Squeezed Attention

This is a solution to the above-described task, where we initially provide a document followed by user queries, implying that most of the context is fixed. The innovation in squeezed attention is the offline optimisation they introduced, which occurs before inference begins.

The method first does K-means clustering on the key vectors of the fixed context, and then groups them based on semantic similarity. This is particularly unique as most methods rely on physical proximity, i.e, position. Additionally, all keys within a single cluster are represented by a centroid, which is called the “key centroid”.

These offline optimisations make the online inference process incredibly efficient. Now, the model only compares the query to the centroids, rather than the entire context, which helps effectively retrieve the relevant clusters and identify only the important keys.

Another important improvement was to represent the clusters with their softmax values, as shown below. This was particularly important because some attention heads exhibited a skewed distribution of attention scores, with fewer important keys compared to balanced attention heads, thereby rendering top-k sampling ineffective. By applying softmax, we can set a fixed threshold, which allows us to dynamically sample keys based on the distribution.

$$
S_i^{(1)} = \frac{\exp\left(q C_i^{(1)\top}\right)}{\sum_j N_j^{(1)} \cdot \exp\left(q C_j^{(1)\top}\right)}
$$

But this clustering algorithm introduces a new centroid lookup cost; therefore, to minimise that, it is ideal to have fewer clusters, thereby making the identification of fine-grained clusters a necessity.

To scale this retrieval efficiently, the authors also propose a hierarchical clustering approach. In this step, the preprocessing is modified as before. First, K-means clustering is performed, and the centroid is defined as the average of the keys. This is defined as “Level 2 clustering”. The next step is to cluster the clusters. K-means is performed again, and now the fine-grained clusters are converted to broader coarse-grained clusters. This is what they define as “Level 1 Clustering.” 

During inference, the query is first compared to level 1 clusters. We then choose relevant clusters based on the target sparsity level we specified (usually 90%). This process is repeated with level 2 clusters, finally leaving us with our important keys.

$$
S_l^{(2)} = \frac{\exp \left(q C_l^{(2)\top}\right)}{\sum_m N_m^{(2)} \cdot \exp \left(q C_m^{(2)\top}\right)}.
$$

However, the squeezed attention method again introduces reliance on hyperparameter tuning, which is dependent on the input context and varies across any dataset with multiple samples. A further optimisation that could be done is to automate the hyperparameter tuning process.

Additionally, the clustering process is very slow, rendering the method ineffective when context is provided online in real-time. Further work can also be done to address this limitation, thereby removing the fixed context requirement.

### Epicache

While Squeezed Attention offers an effective solution for static, long-context documents, it runs into a critical limitation: it relies on "post-prefill" processing. The model must first load the entire context into memory to cluster the keys, leading to unbounded peak memory usage. Which proves to be problematic.

Epicache utilises a block-wise prefill, where less important tokens in each block are pruned using a patched prompt; this is what enables the fixed-budget operation.

Their core insight is that long conversations are not a single topic but are composed of various "episodes."For example, a discussion about video games, followed by planning a movie, followed by an aside about the weather, and this has been termed as “episodes” in the paper.

#### Methodology

##### Stage 1

First, the conversation history H{u_1,u_2…..u_N_u) is partitioned into K segments, where each segment is defined as S_k. Each have a size of w_embed. Note N_u refers to the number of utterances in the conversation history

$$
K = \left\lceil \frac{N_u}{w_{embed}} \right\rceil
$$

Each segment is encoded as an embedding, upon which we perform clustering to get E topical episodes.

$$
C(\{e_k\}_{k=1}^K) \rightarrow \{\mathcal{E}_1, \dots, \mathcal{E}_E\}
$$

For each cluster (Ee) we compute the centroid to represent the cluster in our vector space. However, for KV cache construction, the mediod is used i.e., the segment closest to the centroid of a cluster.​ And the mediod serves as the patched prompt.

$$
s_i^{max} = \max_{t \in [n+1, n+p]} Attn(x_t \to x_i)
$$

##### Stage 2

Block wise prefill proves to be a necessity in this case, as we need to build the KV cache separately for each episode. The context is processed in block of a size M, where the patched prompt is appended to get the importance scores. An attention-based scoring function is used where the importance si​ of a token i is determined by the maximum attention it receives from the patched prompt queries:

$$
s_i^{max} = \max_{t \in [n+1, n+p]} Attn(x_t \to x_i)
$$

**Layer Wise Sensitivity**

The global budget is not distributed uniformly. We measure layer-wise sensitivity σl​ by calculating the cosine similarity between Key states generated under full context versus block-prefill masks. The budget for layer l, denoted (M_l)^alloc​, is proportional to its sensitivity, assigning more capacity to layers that deviate significantly under compression:

$$
M_l^{alloc} = \frac{s_l^\alpha}{\sum s_{l'}^\alpha} (L \cdot M)
$$

To simulate compression, they replace the standard causal mask with a custom mask, this mask only allows the tokens to attend to the Sink tokens and the recent tokens. This test enabled the authors to predict how a layer behaves when it’s access to long-range context is taken away.

This was formulated based on how much the keys change when switching between the masks. (represented by cosine similarity) 

$$
\sigma_l = \frac{1}{H \times N} \sum_{h=1}^{H} \sum_{i=1}^{N} \cos \left( \mathbf{k}_{full,i}^{(l,h)}, \mathbf{k}_{block,i}^{(l,h)} \right)
$$

Therefore, sensitivity was defined as sl​=1−σl.​

α represents the sharpness parameter, it controls the extremity of the budget allocation between sensitive and insensitive layers. For Llama, the ideal value was observed to be 1.1, while for qwen it was 1.3.

##### Stage 3

During decoding, the query is embedded into the same vector space as the clusters, where a similarity search is performed to identify the most relevant episodes. The system then retrieves the pre-computed episodic caches, which are used to generate the output.

---

## Conclusion

Scaling Transformers has always been limited by the Quadratic Bottleneck. Traditionally, the only solution was more compute, but research has revealed that the solution is actually Sparsity, the idea that models don't need to see everything to understand something.

We have seen that this sparsity can be harnessed or overcome in multiple ways, such as: 

1. Mathematical Reformulations: Changing functions like Softmax to fix issues like Attention Sinks and "underscoring" in long contexts. 
2. Architectural Patterns: Using structures like Block-Sparsity or Anchor Blocks to maintain global context without the massive cost. 
3. Memory Management: Moving from vanilla storage to smart caching, whether by tracking "Heavy 

Hitters" at the token level or organizing conversations into "Episodes".
The future of Long Context isn't about more compute. By treating the KV Cache as a dynamic library rather than a static log, we can build systems that reason over large data without crashing memory.


---


